{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/charlie/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from torch import cuda, no_grad\n",
    "import re\n",
    "import html\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class Predictor():\n",
    "    def __init__(self,\n",
    "                 MODEL_DIR=\"models/bert_predictor\",\n",
    "                 MODEL_PATH=\"distilbert-base-cased\",\n",
    "                 DATA_RAW_DIR=\"./data/raw/\",\n",
    "                 DATA_PROC_DIR=\"./data/preprocessed/\",\n",
    "                 EOS_TOKEN='<|endoftext|>',\n",
    "                 SEP_TOKEN='<\\|reply\\|>',\n",
    "                 MAX_LENGTH=512,\n",
    "                 TRAIN_RATIO=0.9,\n",
    "                 BATCH_SIZE=4,\n",
    "                 EPOCHS=1,\n",
    "                 LEARNING_RATE=2e-5,\n",
    "                 SEED=42\n",
    "                 ):\n",
    "              \n",
    "        # Settings\n",
    "        self.MODEL_DIR = MODEL_DIR\n",
    "        self.MODEL_PATH = MODEL_PATH\n",
    "        self.DATA_RAW_DIR = DATA_RAW_DIR\n",
    "        self.DATA_PROC_DIR = DATA_PROC_DIR\n",
    "        self.EOS_TOKEN = EOS_TOKEN\n",
    "        self.SEP_TOKEN = SEP_TOKEN\n",
    "        self.MAX_LENGTH = MAX_LENGTH\n",
    "        self.TRAIN_RATIO = TRAIN_RATIO\n",
    "        self.BATCH_SIZE = BATCH_SIZE\n",
    "        self.EPOCHS = EPOCHS\n",
    "        self.LEARNING_RATE = LEARNING_RATE\n",
    "        self.SEED = SEED\n",
    "\n",
    "        # Device\n",
    "        self.torch_device = \"cuda\" if cuda.is_available() else \"cpu\"\n",
    "        print(\"Using device: \" + self.torch_device)\n",
    "\n",
    "        # Tokenizer + Model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('distilbert-base-cased', do_lower_case=True) # !!!!need to retrain with do_lower_case=False\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(self.MODEL_PATH, num_labels=1).to(self.torch_device)\n",
    "        self.data_collator = DataCollatorWithPadding(tokenizer=self.tokenizer)\n",
    "\n",
    "    def regex_text(self, text):\n",
    "        text = html.unescape(text)\n",
    "        text = re.sub(r\"\\\\'\", r\"'\", text)\n",
    "        text = re.sub(r\"\\s+$\", '', text)  \n",
    "        texts = re.findall(self.SEP_TOKEN + \" (,?.*)\", text)\n",
    "        for t in texts:\n",
    "            if t:\n",
    "                text = t\n",
    "                break\n",
    "        text = text.rstrip()\n",
    "        return text\n",
    "    \n",
    "    def clean_dataframe(self, df):\n",
    "        df['completion'] = df['completion'].astype(str)\n",
    "        df['completion'] = df['completion'].apply(self.regex_text)\n",
    "        df = df[df['completion'].str.len() != 0]\n",
    "        return df\n",
    "    \n",
    "    def minmax_scale(self, X, X_min, X_max):\n",
    "        X_scaled = (X - X_min) / (X_max - X_min)\n",
    "        return X_scaled\n",
    "\n",
    "    # Scaling is [MinMax -> np.exp -> MinMax] such that the comments/replies with a higher count have more influence\n",
    "    # To do: Fit scale to training data only\n",
    "    def scale(self, df, cols=['comment_score', 'reply_score']):\n",
    "        min_score = df[cols].min().min()\n",
    "        print(\"Min score: \", min_score)\n",
    "        max_score = df[cols].max().max()\n",
    "        print(\"Max score: \", max_score)\n",
    "        for col in cols:\n",
    "            df[col+\"_scaled\"] = df[col]\n",
    "            col = col+\"_scaled\"\n",
    "            df[col] = df[col].apply(self.minmax_scale, args=(min_score, max_score))\n",
    "            df[col] = df[col].apply(np.exp)\n",
    "\n",
    "        cols = ['comment_score_scaled', 'reply_score_scaled']\n",
    "        min_score = df[cols].min().min()\n",
    "        print(\"Min score: \", min_score)\n",
    "        max_score = df[cols].max().max()\n",
    "        print(\"Max score: \", max_score)\n",
    "        for col in cols:\n",
    "            df[col] = df[col].apply(self.minmax_scale, args=(min_score, max_score))\n",
    "        return df\n",
    "    \n",
    "    def scale_dataframe(self, df):\n",
    "        df = self.scale(df)\n",
    "        df['reply_score_minmax'] = df['reply_score'].apply(self.minmax_scale, args=(df['reply_score'].min(), df['reply_score'].max()))\n",
    "        df['score_ratio'] = df['reply_score_scaled']/df['comment_score_scaled']\n",
    "        return df\n",
    "    \n",
    "    def tokenize_function(self, examples):\n",
    "        return self.tokenizer(examples[\"completion\"], truncation=True, padding=True, max_length=self.MAX_LENGTH)\n",
    "    \n",
    "    def preprocessing(self):\n",
    "        validation = pd.read_csv(self.DATA_PROC_DIR + '/validation.csv', index_col=0)[['completion', 'comment_score', 'reply_score']]\n",
    "        validation = self.clean_dataframe(validation)\n",
    "        validation = self.scale_dataframe(validation)\n",
    "        validation = validation[['completion', 'reply_score_minmax']].rename(columns={'reply_score_minmax': 'label'})\n",
    "\n",
    "        train = pd.read_csv(self.DATA_PROC_DIR + '/train.csv', index_col=0)[['completion', 'comment_score', 'reply_score']]\n",
    "        train = self.clean_dataframe(train)\n",
    "        train = self.scale_dataframe(train)\n",
    "        train = train[['completion', 'reply_score_minmax']].rename(columns={'reply_score_minmax': 'label'})\n",
    "\n",
    "        dataset = dict()\n",
    "        dataset['validation'] = Dataset.from_pandas(validation, preserve_index=False)\n",
    "        dataset['train'] = Dataset.from_pandas(train, preserve_index=False)\n",
    "        datasets = DatasetDict(dataset)\n",
    "\n",
    "        tokenized_datasets = datasets.map(\n",
    "            self.tokenize_function,\n",
    "            batched=True,\n",
    "            num_proc=1,\n",
    "            remove_columns=[\"completion\"],\n",
    "            )\n",
    "\n",
    "        return tokenized_datasets\n",
    "    \n",
    "    def train_model(self, dataset, SAVE_STEPS=10000, model_name=None):\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=self.MODEL_DIR,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            learning_rate=self.LEARNING_RATE,\n",
    "            weight_decay=0.01,\n",
    "            per_device_train_batch_size=self.BATCH_SIZE,\n",
    "            per_device_eval_batch_size=self.BATCH_SIZE,\n",
    "            num_train_epochs=self.EPOCHS,\n",
    "            save_steps=SAVE_STEPS,\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=dataset[\"train\"],\n",
    "            eval_dataset=dataset[\"validation\"],\n",
    "            data_collator=self.data_collator,\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "\n",
    "        if model_name:\n",
    "            trainer.save_model(self.MODEL_DIR + \"/\" + model_name)\n",
    "        else:\n",
    "            now = datetime.now()\n",
    "            dt_string = now.strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "            trainer.save_model(self.MODEL_DIR + \"/model-\" + dt_string)\n",
    "\n",
    "    def run_training_pipeline(self):\n",
    "        dataset = self.preprocessing()\n",
    "        self.train_model(dataset)\n",
    "\n",
    "    def predict(self, realistic_texts):\n",
    "        scores = []\n",
    "        for i, text in enumerate(realistic_texts):\n",
    "            print(str(i) + \": \" + text)\n",
    "            test_input = self.tokenizer(text, return_tensors='pt').to(self.torch_device)\n",
    "            with no_grad():\n",
    "                output = self.model(**test_input)\n",
    "\n",
    "            scores.append(output.logits[0][0].cpu().numpy())\n",
    "\n",
    "        output_text = realistic_texts[np.argmax(scores)]\n",
    "        return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Min score:  -107\n",
      "Max score:  8401\n",
      "Min score:  1.0\n",
      "Max score:  2.718281828459045\n",
      "Min score:  -119\n",
      "Max score:  8401\n",
      "Min score:  1.0\n",
      "Max score:  2.718281828459045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/charlie/.local/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/31215 [00:00<?, ?it/s]You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "  0%|          | 61/31215 [00:23<3:18:57,  2.61it/s]"
     ]
    }
   ],
   "source": [
    "predictor = Predictor(MODEL_PATH='./models/bert_predictor/final')\n",
    "predictor.run_training_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
