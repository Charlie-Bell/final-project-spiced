{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/charlie/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, DataCollatorForLanguageModeling, set_seed, TrainingArguments, Trainer\n",
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "import os\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(torch_device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nucleus sampling + Top-K sampling + num sequences = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/charlie/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1259: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: This is my favorite part of the bio class.  \n",
      "\n",
      "1: &gt; My minor was in psych based neuro but I took my bio electives as bio based neuro courses.\n",
      "\n",
      "2: I have no idea what you're talking about. I'm interested to know. \n",
      "\n",
      "3: This is what I think! \n",
      "\n",
      "4: How did your bio pass?  I also knew that I had to pay a lot of money, even though I was very good at a math course. I know that was a good one to pay\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# add the EOS token as PAD token to avoid warnings\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"../output/gpt2/final\", pad_token_id=tokenizer.eos_token_id).to(torch_device)\n",
    "\n",
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "set_seed(42)\n",
    "\n",
    "# encode context the generation is conditioned on\n",
    "sep_token = \"<|reply|>\"\n",
    "input_text = \"Oh nice! I got around this (kind of) by being a bio major with a minor in neuro research. My school had 2 neruo programs - bio based and psych based. My minor was in psych based neuro but I took my bio electives as bio based neuro courses. Still had to do calc 1, calc 2, chem 1, chem 2, orgo 1, orgo 2, physics 1, and physics 2. But I DIDNT have to take intro to pharma kinetics, inorganic chemistry and a few other higher level chem classes. I did this bc the psych based neuro courses had almost no bio and I love bio. But math and chem are my kryptonite.\"\n",
    "model_inputs = tokenizer([\" \".join([input_text, sep_token])], return_tensors='pt').to(torch_device)\n",
    "\n",
    "# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\n",
    "sample_outputs = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=40,\n",
    "    do_sample=True,\n",
    "    early_stopping=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    temperature=0.8,\n",
    "    num_return_sequences=5,\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    text = tokenizer.decode(sample_output, skip_special_tokens=False).split('<|reply|>')[1].split('\\n')[0][1:]\n",
    "    print(f\"{i}: {text}\\n\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "MAX_LENGTH = 512\n",
    "EPOCHS = 2\n",
    "\n",
    "bos_token='<|startoftext|>'\n",
    "eos_token='<|endoftext|>'\n",
    "pad_token='<|endoftext|>'\n",
    "sep_token='<|reply|>'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\", return_tensors='pt', eos_token=eos_token, pad_token=pad_token)\n",
    "\n",
    "# Tokenizer function for later mapping.\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=True, max_length=MAX_LENGTH)\n",
    "\n",
    "def insert_tags(pair):\n",
    "    return \" \".join([eos_token, pair[0], sep_token, pair[1], eos_token])\n",
    "\n",
    "# Create dataset as a DatasetDict object\n",
    "DATA_DIR = '../data/raw/'\n",
    "filenames = os.listdir(DATA_DIR)\n",
    "dfs = [pd.read_csv(DATA_DIR + name, index_col='Unnamed: 0') for name in filenames]\n",
    "df = pd.concat(dfs)\n",
    "def create_dataset(df):\n",
    "    comments = df[['comment', 'reply']]['comment'].apply(str).to_list()\n",
    "    replies = df[['comment', 'reply']]['reply'].apply(str).to_list()\n",
    "    texts = [insert_tags(pair) for pair in zip(comments, replies)]\n",
    "\n",
    "    train_percentage = 0.9\n",
    "    validation_percentage = 0.07\n",
    "    test_percentage = 0.03\n",
    "\n",
    "    random.shuffle(texts)\n",
    "    texts_size = len(texts)\n",
    "    texts_train = texts[:int(train_percentage*texts_size)]\n",
    "    texts_validation = texts[int(train_percentage*texts_size):]\n",
    "\n",
    "    dataset = dict()\n",
    "    dataset['train'] = Dataset.from_dict({'text': texts_train})\n",
    "    dataset['validation'] = Dataset.from_dict({'text': texts_validation})\n",
    "    datasets = DatasetDict(dataset)\n",
    "    return datasets\n",
    "\n",
    "def group_texts(examples):\n",
    "    examples['labels'] = examples['input_ids']\n",
    "    return examples\n",
    "\n",
    "dataset = create_dataset(df)\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    remove_columns=[\"text\"],\n",
    "    )\n",
    "\n",
    "tokenized_dataset.set_format(\"pt\", columns=['input_ids', 'attention_mask'], output_all_columns=True)\n",
    "\n",
    "lm_dataset = tokenized_dataset.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    )\n",
    "\n",
    "lm_dataset.set_format(\"pt\", columns=['input_ids', 'attention_mask', 'labels'], output_all_columns=True)\n",
    "\n",
    "# Data Collator pads the inputs for Causal Language Modeling.\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(lm_dataset['train'][1]['labels'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\").to(torch_device)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../output/gpt2\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    save_steps=10000,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_dataset[\"train\"],\n",
    "    eval_dataset=lm_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
