{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/charlie/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from random import randrange\n",
    "import pandas as pd\n",
    "\n",
    "torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_token='<|endoftext|>'\n",
    "pad_token='<|endoftext|>'\n",
    "sep_token='<|reply|>'\n",
    "\n",
    "# add the EOS token as PAD token to avoid warnings\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\", return_tensors='pt', eos_token=eos_token, pad_token=pad_token)\n",
    "tokenizer.padding_side = \"left\" \n",
    "model = AutoModelForCausalLM.from_pretrained(\"../output/gpt2/final\", pad_token_id=tokenizer.eos_token_id).to(torch_device)\n",
    "\n",
    "datasets = DatasetDict().load_from_disk(\"../data/preprocessed/dataset.hf\")\n",
    "decoded = [tokenizer.decode(tensor, skip_special_tokens=True)[1:-2] for tensor in datasets['validation']['input_ids']]\n",
    "\n",
    "def create_fake(input_texts):\n",
    "    full_texts = input_texts\n",
    "    labels = False\n",
    "    \n",
    "    if randrange(100) >= 50:\n",
    "        prompt_texts = [input_text.split(sep_token)[0]+sep_token for input_text in input_texts]       \n",
    "        model_inputs = tokenizer(prompt_texts, return_tensors='pt', padding=True, truncation=True, max_length=1024).to(torch_device)\n",
    "        # set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\n",
    "        sample_outputs = model.generate(\n",
    "            **model_inputs,\n",
    "            max_new_tokens=40,\n",
    "            do_sample=True,\n",
    "            early_stopping=True,\n",
    "            top_k=50,\n",
    "            top_p=0.8,\n",
    "            temperature=0.95,\n",
    "            #num_return_sequences=1,\n",
    "        )\n",
    "\n",
    "        generated_text = tokenizer.batch_decode(sample_outputs, skip_special_tokens=True)\n",
    "        full_texts = generated_text\n",
    "        labels = True\n",
    "\n",
    "    return full_texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yield successive n-sized\n",
    "# chunks from l.\n",
    "def divide_chunks(decoded, batch_size):\n",
    "      \n",
    "    # looping till length l\n",
    "    for i in range(0, len(decoded), batch_size): \n",
    "        yield decoded[i:i + batch_size]\n",
    "  \n",
    "# How many elements each\n",
    "# list should have\n",
    "batch_size = 10\n",
    "  \n",
    "decoded_batches = list(divide_chunks(decoded, batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/charlie/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1259: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 of 1388\n",
      "Batch 50 of 1388\n",
      "Batch 100 of 1388\n",
      "Batch 150 of 1388\n",
      "Batch 200 of 1388\n",
      "Batch 250 of 1388\n",
      "Batch 300 of 1388\n",
      "Batch 350 of 1388\n",
      "Batch 400 of 1388\n",
      "Batch 450 of 1388\n",
      "Batch 500 of 1388\n",
      "Batch 550 of 1388\n",
      "Batch 600 of 1388\n",
      "Batch 650 of 1388\n",
      "Batch 700 of 1388\n",
      "Batch 750 of 1388\n",
      "Batch 800 of 1388\n",
      "Batch 850 of 1388\n",
      "Batch 900 of 1388\n",
      "Batch 950 of 1388\n",
      "Batch 1000 of 1388\n",
      "Batch 1050 of 1388\n",
      "Batch 1100 of 1388\n",
      "Batch 1150 of 1388\n",
      "Batch 1200 of 1388\n",
      "Batch 1250 of 1388\n",
      "Batch 1300 of 1388\n",
      "Batch 1350 of 1388\n"
     ]
    }
   ],
   "source": [
    "batches_with_fakes = []\n",
    "num_batches = len(decoded_batches)\n",
    "for i, decoded_batch in enumerate(decoded_batches):\n",
    "    batch_with_fake = create_fake(decoded_batch)\n",
    "    if (i%50==0):\n",
    "        print(f\"Batch {i} of {num_batches}\")\n",
    "    batches_with_fakes.append(batch_with_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_fakes_dict = {\n",
    "    'text': [],\n",
    "    'label': [],\n",
    "}\n",
    "\n",
    "for batch in batches_with_fakes:\n",
    "    texts, label = batch[0], batch[1]\n",
    "    for text in texts:\n",
    "        dataset_fakes_dict['text'].append(text)\n",
    "        dataset_fakes_dict['label'].append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=dataset_fakes_dict)\n",
    "df.to_csv('../data/preprocessed/fakes_validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
